OpenCL Programming:
*********************************
Modern computing relies on massive parallelism, where thousands of operations execute simultaneously across diverse hardware platforms. OpenCL (Open Computing Language) enables high-performance computing by providing a unified framework for programming CPUs, GPUs, and FPGAs. This course introduces you to the fundamentals of OpenCL programming, from setting up the development environment to writing and optimizing parallel computing applications. Through hands-on exercises and real-world case studies, you will gain the expertise to develop scalable, high-performance applications that leverage the power of heterogeneous.

This course is designed for professionals and enthusiasts eager to explore high-performance computing and parallel programming using OpenCL. Programmers and software developers working in fields such as scientific computing, gaming, and multimedia processing will find OpenCL essential for optimizing performance across CPUs, GPUs, and FPGAs. GPU programmers looking to develop portable, hardware-agnostic code will benefit from OpenCL’s flexibility in enabling parallel computation across multiple vendors. Additionally, embedded systems engineers can leverage OpenCL to accelerate applications on resource-constrained devices, optimizing performance for real-time processing. Data scientists and researchers engaged in deep learning, simulations, and large-scale data processing will also find OpenCL invaluable for enhancing computational efficiency and scalability.

To get the most out of this course, a solid foundation in C or C++ programming is required, as OpenCL uses a C-based API and kernel development follows C syntax. Learners should be comfortable with memory management, pointers, and function calls. A basic understanding of parallel programming concepts, such as threads, task parallelism, and synchronization, will be beneficial in grasping OpenCL’s execution model. Additionally, familiarity with CPU and GPU architectures—including differences in execution units, memory hierarchies, and computational capabilities—will aid in writing optimized OpenCL programs. Since OpenCL development often involves command-line tools for compiling and running programs, prior experience with CLI environments is recommended. Finally, a strong problem-solving mindset is essential, as OpenCL requires tuning for performance optimization and debugging at a low level.

By the end of this course, you will have a strong grasp of OpenCL programming, enabling you to create high-performance applications that fully leverage parallel computing power across CPUs, GPUs, and other hardware platforms. Whether you're working on machine learning, AI, 3D graphics, or scientific simulations, you'll be equipped to optimize performance and tackle complex computational challenges. Take the next step in advancing your skills and unlocking new opportunities in the rapidly growing field of high-performance computing with OpenCL.

**********************************
****************************************
How Does OpenCL Work?
OpenCL is a programming framework and runtime that enables a programmer to create small programs, called kernel programs (or kernels), that can be compiled and executed, in parallel, across any processors in a system. The processors can be any mix of different types, including CPUs, GPUs, DSPs, FPGAs or Tensor Processors - which is why OpenCL is often called a solution for heterogeneous parallel programming.

The OpenCL framework contains two APIs. The Platform Layer API is run on the host CPU and is used first to enable a program to discover what parallel processors or compute devices are available in a system. By querying for what compute devices are available an application can run portably across diverse systems - adapting to different combinations of accelerator hardware. Once the compute devices are discovered, the Platform API enables the application to select and initialize the devices it wants to use.

The second API is the Runtime API which enables the application's kernel programs to be compiled for the compute devices on which they are going to run, loaded in parallel onto those processors and executed. Once the kernel programs finish execution the Runtime API is used to gather the results.



Compiling and Executing OpenCL Kernels


The most commonly used language for programming the kernels that are compiled and executed across the available parallel processors is called OpenCL C. OpenCL C is based on C99 and is defined as part of the OpenCL specification. Kernels written in other programming languages may be executed using OpenCL by compiling to an intermediate program representation, such as SPIR-V.

OpenCL is a low-level programming framework so the programmer has direct, explicit control over where and when kernels are run, how the memory they use is allocated and how the compute devices and host CPU synchronize their operations to ensure that data and computed results flow correctly - even when the host and compute kernels are running in parallel.

Executing an OpenCL Program
OpenCL regards a kernel program as the basic unit of executable code (similar to a C function). Kernels can execute with data or task-parallelism. An OpenCL program is a collection of kernels and functions (similar to dynamic library with run-time linking).

An OpenCL command queue is used by the host application to send kernels and data transfer functions to a device for execution. By enqueueing commands into a command queue, kernels and data transfer functions may execute asynchronously and in parallel with application host code.

The kernels and functions in a command queue can be executed in-order or out-of-order. A compute device may have multiple command queues.



Sequence for Executing OpenCL Kernels


A complete sequence for executing an OpenCL program is:

Query for available OpenCL platforms and devices

Create a context for one or more OpenCL devices in a platform

Create and build programs for OpenCL devices in the context

Select kernels to execute from the programs

Create memory objects for kernels to operate on

Create command queues to execute commands on an OpenCL device

Enqueue data transfer commands into the memory objects, if needed

Enqueue kernels into the command queue for execution

Enqueue commands to transfer data back to the host, if needed


************************
Programming OpenCL Kernels
An OpenCL application is split into host code and device kernel code. Host code is typically written using a general programming language such as C or C++ and compiled by a conventional compiler for execution on the host CPU. OpenCL bindings for other languages are also available, such as Python.

Device kernels that are written in OpenCL C, which is based on C99, can be ingested and compiled by the OpenCL driver during execution of an application using runtime OpenCL API calls. This is called online compilation and is supported by all OpenCL drivers. OpenCL C is a subset of ISO C99 with language extensions for parallelism, well-defined numerical accuracy (IEEE 754 rounding with specified max error) and a rich set of built-in functions including cross, dot, sin, cos, pow, log etc.




Traditional Versus OpenCL Programming Using OpenCL C Kernels


The OpenCL specification also enables optional offline compilation where the kernel program is pre-compiled into a binary format that a particular driver can ingest. Offline compilation can add significant value to developers by:

Speeding OpenCL application execution by eliminating or minimizing kernel code compilation time.
Leveraging alternative kernel languages and tools to produce executable binaries.
There are two offline compilation approaches:

Kernels that are compiled online by the driver can be retrieved by the application using the clGetProgramInfo call. Those cached, device-specific, kernels can then be later reloaded for execution on the same device instead of re-compiling those kernels from the source code.
Offline compilers can be invoked independently before the OpenCL application executes to generate binaries to load and run on the device during application execution.



Online Versus Offline Compilation of Kernels


Early OpenCL implementations primarily used proprietary binary formats and caching of driver-compiled binaries to achieve offline compilation. However, the binaries created by the compiler in a specific device driver are not portable to other devices, and so applications using cached binaries lost the portbility to any device that is possible through online compilation of OpenCL C. To solve this portability problem, and to enable a richer language and compiler ecosystem, Khronos has defined a cross-vendor, portable intermediate program representation called SPIR-V. An increasing number of OpenCL implementations are supporting ingestion of offline-compiled kernel programs in the SPIR-V format.

SPIR-V enables independent innovation by the compiler and silicon communities. Compiler front ends that generate SPIR-V kernels that can be ingested and executed by any OpenCL driver that understands the SPIR-V format. For example the C++ for OpenCL open source front-end and compilers for SYCL can generate SPIR-V code. Both languages bring C++ functionality to programming OpenCL. While C++ for OpenCL allows using C++ features in the traditional OpenCL kernel code, SYCL provides single-source C++ solution both for the host code and the kernel code. There is also ongoing work on providing SPIR-V support in non C/C++-based languages e.g. Julia.

SPIR-V also enables OpenCL kernels written in OpenCL C and C++ for OpenCL to be executed by runtimes other than OpenCL, providing more deployment flexibility for developers that have invested in OpenCL kernel programming. For example, the Google clspv open source compiler can generate Vulkan SPIR-V shaders from OpenCL C kernel source code, Microsoft is working on a compiler chain that can ingest OpenCL SPIR_V kernels into DX12, and there is early work on using SPIR-V tools such as SPIRV-Cross to bring OpenCL SPIR-V kernels into Metal for execution on Apple platforms.

This diverse OpenCL language ecosystem in turn gives a rich choice to domain specific languages, such as the Halide image processing framework, that can compile to OpenCL C kernels either using source-to-source translations or generating SPIR-V.




OpenCL Language Ecosystem Enabled With SPIR-V

********************************************
OpenCL Programming Model
To understand how to program OpenCL in more detail let's consider the Platform, Execution and Memory Models. The three models interact and define OpenCL's essential operation.

Platform Model
The OpenCL Platform Model describes how OpenCL understands the compute resources in a system to be topologically connected.

A host is connected to one or more OpenCL compute devices. Each compute device is collection of one or more compute units where each compute unit is composed of one or more processing elements. Processing elements execute code with SIMD (Single Instruction Multiple Data) or SPMD (Single Program Multiple Data) parallelism.




OpenCL Platform Model


For example, a compute device could be a GPU. Compute units would then correspond to the streaming multiprocessors (SMs) inside the GPU, and processing elements correspond to individual streaming processors (SPs) inside each SM. Processors typically group processing elements into compute units for implementation efficiency through sharing instruction dispatch and memory resources, and increasing local inter-processor communication.

Execution Model
OpenCL's clEnqueueNDRangeKernel command enables a single kernel program to be initiated to operate in parallel across an N-dimensional data structure. Using a two-dimensional image as a example, the size of the image would be the NDRange, and each pixel is called a work-item that a copy of kernel running on a single processing element will operate on.

As we saw in the Platform Model section above, it is common for processors to group processing elements into compute units for execution efficiency. Therefore, when using the clEnqueueNDRangeKernel command, the program specifies a work-group size that represents groups of individual work-items in an NDRange that can be accommodated on a compute unit. Work-items in the same work-group are able to share local memory, synchronize more easily using work-group barriers, and cooperate more efficiently using work-group functions such as async_work_group_copy that are not available between work-items in separate work-groups.




A 2D Image as an Example NDRange


Memory Model
OpenCL has a hierarchy of memory types:

Host memory - available to the host CPU

Global/Constant memory - available to all compute units in a compute device

Local memory - available to all the processing elements in a compute unit

Private memory - available to a single processing element

*******************************
CMake Build System Support
The Khronos OpenCL ecosystem currently dominantly uses CMake as tool of choice for build automation and unit testing. OpenCL has had facilities for building OpenCL applications long before the the Khronos-hosted OpenCL-SDK manifested.

Dependency detection
CMake has two package detection mechanisms: Find Modules and Package Config files. The prior is for packages typically built using CMake-unaware tooling while the latter provide 1st class integration into CMake build definitions. CMake has had Find Module support for OpenCL for a long time now while the OpenCL SDK added Package Config support, which is encourage for new OpenCL projects. For more information on Search Modes, refer to the CMake docs.

Find Module
For the latest documentation on this method, refer CMake's docs.

CMake 3.1+
FindOpenCL.cmake has been a part of CMake since version 3.1.

cmake_minimum_required(VERSION 3.1)
project(MyApp LANGUAGES C)
find_package(OpenCL REQUIRED)
add_executable(${PROJECT_NAME} Main.c)
target_include_directories(${PROJECT_NAME} PRIVATE ${OpenCL_INCLUDE_DIRS})
target_link_libraries(${PROJECT_NAME} PRIVATE ${OpenCL_LIBRARIES})
Builds a simple OpenCL application. The find_package(OpenCL REQUIRED) function call will run the script CMake-hosted FindOpenCL.cmake script which will look for typical vendor SDK installations. The user may guide detection using both CMake variables and environmental variables.

CMake 3.7+
CMake 3.7 added the more modern IMPORTED target OpenCL::OpenCL which bundles all the necessary compiler switches in one entity.

cmake_minimum_required(VERSION 3.7)
project(MyApp LANGUAGES C)
find_package(OpenCL REQUIRED)
add_executable(${PROJECT_NAME} Main.c)
target_link_libraries(${PROJECT_NAME} PRIVATE OpenCL::OpenCL)
Intention is that Kitware's CMake-hosted FindOpenCL.cmake with time export the same imported targets as the Khronos-hosted Package Config scripts.

Package Config
The Khronos-hosted OpenCL SDK improves on the Find Module approach by making detection more robust and adding more control over detecting parts of the required components.

cmake_minimum_required(VERSION 3.0)
project(MyApp LANGUAGES C)
find_package(OpenCL REQUIRED)
add_executable(${PROJECT_NAME} Main.c)
target_link_libraries(${PROJECT_NAME} PRIVATE OpenCL::OpenCL)
The script above builds a simple OpenCL application. Note that script is identical to the IMPORTED target version, but doesn't require CMake 3.7.

This script using the Basic Signature of find_package() is able to detect the minimal set of OpenCL development files using both the Find Module and Package Config search modes. There is one important caveat to using this simplest form: CMake favors Module mode over Config mode and should the user have some GPGPU SDK installed or has OpenCL development files in system default locations, it will always "hide" a custom built OpenCL SDK. Preferring custom packages over system ones is possible with setting the CMAKE_FIND_PACKAGE_PREFER_CONFIG variable on the command-line. (Do not bake this into scripts, it is user preference!)

The OpenCL SDK defines the following IMPORTED targets as components:

OpenCL::Headers for the C API headers
OpenCL::HeadersCpp for the C++ API headers (depends on the C Headers)
OpenCL::OpenCL for the ICD Loader (depends on the C Headers)
OpenCL::Utils for the C Utility library
OpenCL::UtilsCpp for the C++ Utility library
Package manager support
When one doesn't consume the SDK from a vendor SDK nor does one build it from source, it's possible to install it via popular package managers.

Vcpkg
For a complete guide on Vcpkg, refer to the product page.

The short summary is that Vcpkg is a Microsoft and community maintained cross-platform open source tool and repository of packages. The packages are called Ports, and the non-metadata part of a Port is a portfile. The portfile is a CMake script (run by the Vcpkg executable using cmake -P in Script Mode) which downloads, patches a project if necessary and builds it from source. To author packages one need not know any other scripting language beside CMake. To check how Vcpkg builds and installs the OpenCL SDK, one may consult OpenCL's portfile.

To build and install the OpenCL SDK, a typical vcpkg CLI invocation would be:

vcpkg install opencl
(For a complete guide on what triplets are, how to select compilers, bitness, static/dynamic library flavors, etc. refer to the product docs. Windows users for eg. typically want to add --triplet=x64-windows to the command-line or set the VCPKG_DEFAULT_TRIPLET environmental variable)

The CMake script samples above need no changes, build scripts are agnostic to consuming the SDK from the system, from a custom build or from Vcpkg. The only thing needing an update is the CMake command-line invocation. By adding -D CMAKE_TOOLCHAIN_FILE=<VCPKGROOT>\scripts\buildsystems\vcpkg.cmake -D  to the CMake command-line we "instruct CMake to use Vcpkg". Explicitly specifying the installed package pool of a triplet is done by adding for eg. -D VCPKG_TARGET_TRIPLET=x64-windows.

Vcpkg CMake integration
A cornerstone feature of Vcpkg is how it integrates into standard CMake workflow. CMake by default has no knowledge of where to look for our OpenCL SDK package obtained by Vcpkg. On platforms like Windows, there aren't even system include/lib directories to search. We could instruct every package individually where to locate itself, in the case of OpenCL for eg. via providing -D OpenCL_INCLUDE_DIR=C:\Users\<username>\Source\Repos\vcpkg\installed\x64-windows\include -D OpenCL_LIBRARY=C:\Users\<username>\Source\Repos\vcpkg\installed\x64-windows\debug\lib\OpenCL.lib on the CMake invocation. This very soon would become tedious.

CMake has a notion of Toolchain Files which can be used to set canonical variables to compiler/linker/etc. executables to override CMake's search mechanism. Toolchain files are loaded very early during configuration (even before the first line of CMakeLists.txt). Vcpkg (ab)uses this mechanism not to define a toolchain, but to hijack much of CMake's internal machinery, including finding packages. Vcpkg appends it's own install directories to CMAKE_PREFIX_PATH and similar variables so find_package calls find packages installed by Vcpkg. Moreover it wraps/overrides the entire find_package command, because some libraries need extra rituals to consume. One such library is Boost, which is wrapped like this taking Vcpkg-specific variables and turning them into Boost_COMPILER and similar variables before issuing the real find_package command.

Vcpkg doesn't monopolize Toolchain Files, by providing VCPKG_CHAINLOAD_TOOLCHAIN_FILE one can specify a real toolchain file which will be loaded after Vcpkg's own. And that's all Vcpkg is.


***********************************
Getting started with OpenCL on Ubuntu Linux
OpenCL is included in all the main distributions of Linux. In this guide we choose Ubuntu as one of the most widespread variants, but all of the distributions have some sort of centralized system of package maintaining and OpenCL realizations can be installed from there.

In this guide we're minimally going to use the following tools:

The command-line
C/C++ compiler
And depending on which ways one extends the bare minimum:

CMake (Cross-platform Make)
Git
Vcpkg (Cross-platform pacakge management)
Visual Studio Code
Steps will be provided to obtain a minimal and productive environment.

Installation
In this guide we'll be using latest (at the time of writing) Ubuntu 20.04 LTS. Installation for the most part will happen via apt (Advanced Packaging Tool), the definitive command-line tool for installing software on Debian-base Linux distribution. It is installed with the system automatically.

(NOTE: installation commands should be issued with root access, so that use of privilege-escalation by sudo is mandatory.)

First update your system. Open your favorite shell in your favorite terminal and run:

sudo apt update
sudo apt upgrade
C/C++ compiler
Open your favorite shell in your favorite terminal and run:

sudo apt install build-essential -y
This will install canonical GNU compiler set g++/GNU 9.3, debugger gdb, as well as some tools (e.g. make, binutils) and libraries for compilation and linking of programs.

Git
You most likely already have Git installed. If not, we'll install it, because this is what is needed to keep your repositories up-to-date. Open your favorite shell in your favorite terminal and run:

sudo apt install git -y
CMake
If you do not have CMake installed yet, we'll install it, as it's the primary supported build system, and it will make our builds much simpler. Open your favorite shell in your favorite terminal and run:

sudo apt install cmake -y
Visual Studio Code
While IDEs are highly opinionated, for an IDE with widespread adoption and small footprint, VS Code has everything (and more) to get off the ground. It can be installed in two ways. The first way is to use the Ubuntu Software GUI, where you can search for code and install it in one click. If you are a fan of command-line tools, use the second method: open your favorite shell in your favorite terminal and run

sudo snap install --classic code
OpenCL-SDK
To build native OpenCL applications, one will minimally need:

C or C++ compiler
The OpenCL headers
The C and optionally the C++ headers
An Installable Client Driver (ICD) Loader
Shared objects library (libOpenCL.so)
The easiest way to obtain these files is using the system package manager, albeit may not be the latest on non-rolling distributions. Cutting-edge versions may be obtained using C/C++ package managers (like Vcpkg) or can be built on-demand from the canonical GitHub-hosted repositories. Git submodules of the SDK include the headers, both C and C++, and Khronos canonical ICD loader, that are built and installed as parts of SDK.

APT
sudo apt install opencl-headers ocl-icd-opencl-dev -y
GitHub
Vcpkg
Compiling on the command-line
Invoking the compiler manually
Compilers native to *nix OS flavors work just out of the box. Then navigate to the folder where you wish to build your application. Our application will have a single Main.c source file:

// C standard includes
#include <stdio.h>

// OpenCL includes
#include <CL/cl.h>

int main()
{
    cl_int CL_err = CL_SUCCESS;
    cl_uint numPlatforms = 0;

    CL_err = clGetPlatformIDs( 0, NULL, &numPlatforms );

    if (CL_err == CL_SUCCESS)
        printf("%u platform(s) found\n", numPlatforms);
    else
        printf("clGetPlatformIDs(%i)\n", CL_err);

    return 0;
}
Then invoke the compiler to build our source file as such:

APT
gcc -Wall -Wextra -D CL_TARGET_OPENCL_VERSION=100 Main.c -o HelloOpenCL -lOpenCL
What do the command-line arguments mean?

-Wall -Wextra turns on all warnings (highest sensible level)
-D instructs the preprocessor to create a define with NAME:VALUE
CL_TARGET_OPENCL_VERSION enables/disables API functions corresponding to the defined version. Setting it to 100 will disable all API functions in the header that are newer than OpenCL 1.0
-I sets additional paths to the include directory search paths
Main.c is the name of the input source file
-o sets the name of the output executable (default would be a.out)
-l instructs linker to link library OpenCL
GitHub
Vcpkg
Running our executable by ./HelloOpenCL either prints the number of platforms found or an error code which is often the result of corrupted or absent runtime installations.

Automating the build using CMake
The CMake build script for this application that builds it as an ISO C11 app with most sensible compiler warnings turned on looks like:

cmake_minimum_required(VERSION 3.1) # 3.1 << C_STANDARD 11

project(HelloOpenCL LANGUAGES C)

find_package(OpenCL REQUIRED)

add_executable(${PROJECT_NAME} Main.c)

target_link_libraries(${PROJECT_NAME} PRIVATE OpenCL::OpenCL)

set_target_properties(${PROJECT_NAME} PROPERTIES C_STANDARD 11
                                                 C_STANDARD_REQUIRED ON
                                                 C_EXTENSIONS OFF)

target_compile_definitions(${PROJECT_NAME} PRIVATE CL_TARGET_OPENCL_VERSION=100)
What does the script do?

Give a name to the project and tell CMake to only look for a C compiler (default is to search for a C and a C++ compiler)
Look for an OpenCL SDK and fail if not found
If detection fails, refer to the CMake Build-System Support chapter.
Specify our source files and name the executable
Specify dependency to the SDK (not just linkage)
Set language properties to all source files of our application
Set the OpenCL version target to control API coverage in header
To invoke this script, place it next to our Main.c file in a file called CMakeLists.txt. Once that's done, CMake may be invoked the following way to generate makefiles in the advised out-of-source fashion into a subfolder named build:

APT
cmake -S . -B ./build
GitHub
Vcpkg

Which will output something like

-- The C compiler identification is GNU 9.3.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ivan/OCL/build
To kick off the build, one may use CMakes build driver:

cmake --build ./build --config Release
Once build is complete, we can run the program by typing:

./build/HelloOpenCL

*********************************
https://github.com/KhronosGroup/OpenCL-Guide?tab=readme-ov-file
https://ulhpc-tutorials.readthedocs.io/en/latest/gpu/opencl/
https://www.coursera.org/payments/checkout?cartId=630091985
**************************
What's OpenCL?
OpenCL came as a standard for heterogeneous programming that enables a code to run in different platforms, such as multicore CPUs, GPUs (AMD, Intel, ARM), FPGAs, Apple M1, tensor cores, and ARM processors with minor or no modifications.

Furthermore, differently from OpenACC, the programmer has full control of the hardware and is entirely responsible for the parallelization process.

However, this portability has a cost, that’s the reason why OpenCL exposes the programmer to a much lower level compared to OpenACC or even CUDA.

OpenCL's target audience:
The target audience of OpenCL consists of programmers that aim at programming portable heterogeneous code and that want full control of the parallelization process.

In this introductory tutorial, we teach how to perform the sum of two vectors C=A+B on the OpenCL device and how to retrieve the results from the device memory.

Objectives of this tutorial:
The main objective of this tutorial is to introduce for students of the HPC school the heterogeneous programming standard - OpenCL. A secondary objective is to show what is behind the higher-level heterogeneous programming libraries, so it is possible to understand how they work.

This tutorial covers the following aspects of OpenCL programming:
Check for OpenCL-capable device(s);
Memory allocation on the device;
Data transfer to the device;
Retrieve data from the device;
Compile C/C++ programs that launch OpenCL kernels.
References:
This tutorial is based on the following content from the Internet:

Tutorial: Simple start with OpenCL and C++
Khronos OpenCL Working Group. The OpenCL Specification (Oct. 2021)
Smistad, E. Getting started with OpenCL and GPU Computing, Feb. 22, 2018 (Access on Oct. 28, 2021).
Mattson, T., McIntosh-Smith, S., Koniges, A. OpenCL: a Hands-on Introduction
Programming Pre-requisites:
C/C++ language and, due to the lower level of OpenCL,
(preferable) knowledge in other frameworks for heterogeneous programming, such as CUDA, OpenACC or SYCL.
Connecting to a GPU-enable node on the Iris Cluster
Ensure you can connect to the UL HPC clusters. In particular, recall that the module command is not available on the access frontends.

Access to the ULHPC iris cluster (here it is the only one featuring GPU nodes):

(laptop)$>  ssh iris-cluster 
Now you'll need to pull the latest changes in your working copy of the ULHPC/tutorials you should have cloned in ~/git/github.com/ULHPC/tutorials (see "preliminaries" tutorial)

(access)$> cd ~/git/github.com/ULHPC/tutorials
(access)$> git pull
Accessing a GPU-equipped node of the Iris cluster
This practical session assumes that you reserve a node on iris with one GPU for interactive development. See documentation

### Have an interactive GPU job
# ... either directly
(access)$> si-gpu
# ... or using the HPC School reservation 'hpcschool-gpu' if needed  - use 'sinfo -T' to check if active and its name
# (access)$> si-gpu --reservation=hpcschool-gpu
$ nvidia-smi
Driver is loaded, but we still need to load the CUDA development kit from the latest software set.

$ resif-load-swset-devel
$ module load system/ULHPC-gpu/2020b
Verifying the OpenCL Installation
First of all, it is necessary to verify if there is an OpenCL-capable device correctly installed by using the clinfo command. This command is not available on the Iris cluster.

If OpenCL is correctly installed, the output of clinfo should return the number of OpenCL-capable devices, the OpenCL version and the name(s) of the device(s), as one can see below.

This is an image

In this example of output, the OpenCL 3.0 version from the CUDA toolkit is installed. Moreover, there is one OpenCL-capable device, a NVIDIA Quadro RTX 5000 GPU.

Refer to the code folder for the complete example.

The OpenCL platform model
The platform model of OpenCL is similar to the one of the CUDA programming model. In short, according to the OpenCL Specification, "The model consists of a host (usually the CPU) connected to one or more OpenCL devices (e.g., GPUs, FPGAs). An OpenCL device is divided into one or more compute units (CUs) which are further divided into one or more processing elements (PEs). Computations on a device occur within the processing element"

An OpenCL program consists of two parts: host code and device code. As the name suggests, the host code is executed by the host and also "submits the kernel code as commands from the host to OpenCL devices".

Finally, such as in the CUDA programming model, the host communicates with the device(s) through the global memory of the device(s). As in the CUDA programming model, there is a memory hierarchy on the device. However, we have omitted these details for the sake of greater simplicity.

Writing a First OpenCL Program
In this tutorial, we will learn how to perform C = A+B in OpenCL.

The initial step is to add the OpenCL headers to the code.

Headers:
For the sake of greater readability, the examples are written using the C++ bindings of OpenCL (<CL/cl.hpp>).

#define CL_USE_DEPRECATED_OPENCL_2_0_APIS
#include <CL/cl.hpp>
Verifying the installed OpenCL platforms and setting up a device
One of the key features of OpenCL is its portability. So, for instance, there might be situations in which both the CPU and the GPU can run OpenCL code. Thus, a good practice is to verify the OpenCL platforms (cl::Platform) to choose on which the compiled code will run.


    cl::Platform default_platform = all_platforms[0];
    std::cout << "Using platform: " <<default_platform.getInfo<CL_PLATFORM_NAME>() << "\n";

An OpenCL platform might have several devices. The next step is to ensure that the code will run on the first device (device 0) of the platform, if found.

  std::vector<cl::Device> all_devices;
    default_platform.getDevices(CL_DEVICE_TYPE_ALL, &all_devices);
    if (all_devices.size() == 0) {
        std::cout << " No devices found.\n";
        exit(1);
    }

    cl::Device default_device = all_devices[0];
    std::cout << "Using device: " << default_device.getInfo<CL_DEVICE_NAME>() << "\n";

The OpenCL Context
According to the OpenCL Parallel Programming Development Cookbook, "contexts are used by the OpenCL runtime for managing objects such as command queues (the object that allows you to send commands to the device), memory, program, and kernel objects, and for executing kernels on one or more devices specified in the context".

    cl::Context context({default_device});
Now we need to define the code object which is going to be executed on the device, called kernel.

   cl::Program::Sources sources;
Allocating Memory on the Device
The host communicates with the device by using its global memory. However, it is required, first, to allocate, on the host portion of the code, to allocate memory on the device. In OpenCL, the memory allocated on the device is called Buffer.

In our example, we have to host vectors, A and B of SIZE=10.

int A_h[] = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 };
int B_h[] = { 10, 9, 8, 7, 6, 5, 4, 3, 2, 1 };
The next step is to allocate two regions on the device memory for A_h, B_h and C_h. It is a good practice to name the variables on the GPU with a suffix _d and _h on the host. This way, A_h is the version of the vector on the host and A_d its copy on the device.

cl::Buffer A_d(context, CL_MEM_READ_WRITE, sizeof(int) * SIZE);
In this example, buffer A_d is connected to the context, and it has the size of SIZE elements of four bytes (sizeof(int)).

A buffer can be of several types. In this tutorial, we focus on CL_MEM_READ_WRITE and CL_MEM_READ_ONLY. These flags say the actions will perform in the buffer.

In this tutorial, it is necessary to create three buffers: one CL_MEM_READ_ONLY for vector A, one CL_MEM_READ_ONLY for vector B, and a CL_MEM_WRITE_ONLY for vector C.

Creating a Queue
In OpenCL, it is required to create a queue to push commands onto the device. For those who program in CUDA, OpenCL queues are similar CUDA streams.

cl::CommandQueue queue(context, default_device);
Writing into the Device Memory
Once the CommandQueue queue is created, it is possible to execute commands on the device side.

Using the queue, connected to the context and the device default_device, it is possible to initialize the vectors A_d and B_d with the values from A_h and B_h.

queue.enqueueWriteBuffer(buffer_A, CL_TRUE, 0, sizeof(int) * SIZE, A_h);
queue.enqueueWriteBuffer(buffer_B, CL_TRUE, 0, sizeof(int) * SIZE, B_h);
Pay attention that the function is writing from the host to the buffer: enqueueWriteBuffer.

It is not required to initialize vector C as it will receive the values of A+B.

Building the OpenCL Kernel
In OpenCL, there are several ways to build the kernel function and enqueue its execution on the device. Unfortunately, to the best of our knowledge, these ways are lower level than CUDA, for which one just needs to define the block size, number of threads and call the kernel as a function.

In this tutorial, we present a way that programmers familiarized with CUDA might understand. In OpenCL, for the sake of higher portability, the kernel function is presented as a string, which is appended to the program in the runtime, as one can see in the code below.

The kernel simple_add is a substring of the global variable of type std::string kernel_code.

std::string kernel_code =
    "   void kernel simple_add(global const int* A, global const int* B, global int* C){ "
    "       C[get_global_id(0)]=A[get_global_id(0)]+B[get_global_id(0)];                 "
    "   } 

In OpenCL, kernel functions must return void, and the global keyword means that the variable points to the global memory.

Organizing the source code for compilation
Initially, it is required to append the kernel, which is presented here as a string, to the OpenCL source code.

  cl::Program::Sources sources;
  sources.push_back({ kernel_code.c_str(),kernel_code.length() });
Then, we create an OpenCL program, linking the OpenCL source code to the context.

  cl::Program program(context, sources);
Then, the OpenCL code is ready to be compiled in execution time


if (program.build({ default_device }) != CL_SUCCESS) {
        std::cout << " Error building: " << program.getBuildInfo<CL_PROGRAM_BUILD_LOG>(default_device) << "\n";
        exit(1);
    }

It is important to point out that compilation errors are found, in execution time at this point. If compilation errors are found, the program output the message Error building, and, then, outputs the compilation errors.

It is worth to mention that there are other ways to define a kernel in OpenCL. Furthermore, it is also possible to perform offline compilation of OpenCL kernels.

Launching the kernel on the device
From the program, which contains the simple_add kernel, create a kernel for execution with three cl:buffers as arguments.


    cl::make_kernel<cl::Buffer, cl::Buffer, cl::Buffer> simple_add(cl::Kernel(program, "simple_add"));
    cl::NDRange global(SIZE);
    simple_add(cl::EnqueueArgs(queue, global), A_d, B_d, C_d).wait();

Next, it is possible to call simple_add as a function. However, note that in cl::EnqueueArgs(queue, global) the queue queue is passed and cl::NDRange global(SIZE) is the number of threads the execution is going to spawn on the device. The remaining arguments are the three buffers on the global memory of the device.

Kernel execution on the device
The kernel is a simple function that performs C[i]=A[i]+B[i]. In this case, OpenCL provides the function get_global_id(0), which returns the id i of the thread in a 1D organization.

Retrieving data from the device
In this example, it is only required to retrieve data from C_d to C_h.

queue.enqueueReadBuffer(C_d, CL_TRUE, 0, sizeof(int) * SIZE, C_h);
In the line above, we read, from the buffer C_d, sizeof(int) * SIZE bytes using the enqueueReadBuffer function.

Compiling an OpenCL code:
It is simple to compile an OpenCL code with gcc or g++. In short, it is only required to add -lOpenCL flag to the compilation command.

If it is not possible to find the OpenCL library, it is required, first, to locate libOpenCL and append the directory to the LD_LIBRARY_PATH or explicitly indicate the location of libOpenCL in the compilation commands.

g++ -lOpenCL exercise1.cpp
Exercise 1: whats the output of exercise1.cpp on the Iris cluster? Is it using the GPUs by Default?
On the Iris cluster, if the programmer compiles the code by only adding the -lOpenCL flag, the compiler uses the Portable Computing Language (PoCL) implementation of OpenCL that runs on the CPU.

So, in the output, this first example, the platform is the Portable Computing Language the device is pthread-Intel(R) Xeon(R) Gold 6132 CPU @ 2.60GHz. For the purpose of this tutorial, it is OK to use the PCL implementation of OpenCL.

If one wants to program the example for the NVIDIA GPUs, it is required to manually point the include and the library directories for the compiler:

g++ -I/mnt/irisgpfs/apps/resif/iris/2019b/gpu/software/CUDA/10.1.243-GCC-8.3.0/include -L/mnt/irisgpfs/apps/resif/iris/2019b/gpu/software/CUDA/10.1.243-GCC-8.3.0/lib64 -lOpenCL exercise1.cpp
Here /mnt/… is the directory in which the module has been loaded.

Exercise 2: compile the code using the NVIDIA implementation of OpenCL.
Compilation errors
For the sake of higher portability, the OpenCL code is compiled in execution time. For instance, in the kernel, adding AAA to the const keyword returns the following compiler error in execution time:

Error building: <kernel>:1:34: error: unknown type name 'constAAA'
   void kernel simple_add(global constAAA int* A, global const int* B, global int* C){        C[get_global_id(0)]=A[get_global_id(0)]+B[get_global_id(0)];                    }                                                                               
...
It is important to point out that this runtime error was returned using the NVIDIA implementation of OpenCL 3.0.

Exercise 3: D=A+B+C
Extend the previous example so a vector D receives the sum of three other vectors, A,B, and C.

Refer to the code folder for the solution.

************************
*********************************


OpenCL Memory Model


OpenCL memory management is explicit. None of the above memories are automatically synchronized and so the application explicitly moves data between memory types as needed.



*******************************
OpenCL basics
OpenCL for Heterogeneous Computing:: each searte basic exmaple code separate
OpenCL (Open Computing Language) is an open standard for writing programs that execute across heterogeneous platforms, including CPUs, GPUs, and FPGAs. It splits code into two parts: the Host code (runs on your CPU) and the Kernel code (runs on the accelerator device). 
The Khronos Group
The Khronos Group
 +4
1. Kernel Code (The "Device" Side) 
The kernel is the function that actually performs the parallel computation. In this example, each "work-item" (thread) computes one element of a vector addition. 
Stack Overflow
Stack Overflow
 +2
c
// Save this as vector_add.cl
__kernel void vector_add(__global const float* A, 
                         __global const float* B, 
                         __global float* C) {
    // Get the unique ID for this specific thread
    int i = get_global_id(0);
    
    // Perform the addition for this element
    C[i] = A[i] + B[i];
}
Use code with caution.

__kernel: Marks the function as an entry point for the device.
__global: Specifies that the data resides in the device's global memory.
get_global_id(0): Returns the index of the current work-item in the 1D range. 
eunomia.dev
eunomia.dev
 +1
2. Host Code (The "CPU" Side)
The host code manages the device, compiles the kernel at runtime, and handles data transfers. Below is a simplified workflow using the OpenCL C++ Wrapper for better readability. 
Read the Docs
Read the Docs
 +4
cpp
#include <CL/cl2.hpp>
#include <vector>
#include <iostream>

int main() {
    // 1. Setup: Get platform and device
    std::vector<cl::Platform> platforms;
    cl::Platform::get(&platforms);
    cl::Platform platform = platforms[0]; // Use the first available platform

    std::vector<cl::Device> devices;
    platform.getDevices(CL_DEVICE_TYPE_GPU, &devices);
    cl::Device device = devices[0]; // Use the first GPU

    // 2. Create Context and Command Queue
    cl::Context context(device);
    cl::CommandQueue queue(context, device);

    // 3. Build Kernel Program from source string
    std::string kernel_code = "__kernel void vector_add(__global const float* A, ...)"; 
    cl::Program program(context, kernel_code);
    program.build({device});

    // 4. Prepare Data and Device Buffers
    int N = 1024;
    std::vector<float> h_A(N, 1.0f), h_B(N, 2.0f), h_C(N);
    
    cl::Buffer d_A(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, sizeof(float)*N, h_A.data());
    cl::Buffer d_B(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, sizeof(float)*N, h_B.data());
    cl::Buffer d_C(context, CL_MEM_WRITE_ONLY, sizeof(float)*N);

    // 5. Execute Kernel
    cl::Kernel kernel(program, "vector_add");
    kernel.setArg(0, d_A);
    kernel.setArg(1, d_B);
    kernel.setArg(2, d_C);
    queue.enqueueNDRangeKernel(kernel, cl::NullRange, cl::NDRange(N), cl::NullRange);

    // 6. Read Results Back
    queue.enqueueReadBuffer(d_C, CL_TRUE, 0, sizeof(float)*N, h_C.data());
    
    std::cout << "Result[0]: " << h_C[0] << std::endl; // Should print 3.0
    return 0;
}
Use code with caution.

Key Differences from Standard Coding
Runtime Compilation: Unlike C++, OpenCL kernels are compiled while the program is running, allowing the same code to run on an NVIDIA GPU, an AMD GPU, or an Intel CPU without being recompiled manually.
Explicit Memory: You must manually move data from "Host RAM" to "Device VRAM" using Buffers.
NDRange: Instead of a for loop, you define an NDRange (the total number of items to process), and the hardware launches that many threads in parallel. 
eunomia.dev
eunomia.dev
 +4
Would you like a guide on how to install the OpenCL SDK for your specific hardware (NVIDIA, AMD, or Intel)?


****************************************
